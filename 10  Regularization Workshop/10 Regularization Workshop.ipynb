{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop - Regularization\n",
    "\n",
    "In this workshop, we are going to:\n",
    "\n",
    "1. Tune an elastic-net regression \n",
    "2. Compare the following models:\n",
    "    1. The null model\n",
    "    2. The tuned elastic-net model\n",
    "    3. The trimmed non-regularized model with standardized features\n",
    "    4. The trimmed non-regularized model with non-standardized features\n",
    "    \n",
    "# Preliminaries\n",
    "\n",
    "- Load any necessary packages and/or functions\n",
    "- Load in and prepare the class data\n",
    "- Create x and y with a label of `pct_d_rgdp`\n",
    "- Create `x_train`, `x_test`, `y_train`, `y_test` with\n",
    "    * training size of two-thirds\n",
    "    * random state of 490\n",
    "- Standardize the features\n",
    "- Add constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('class_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['fips','GeoName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped = df.drop(columns = ['urate_bin', 'year']).join([\n",
    "    pd.get_dummies(df['urate_bin'], drop_first = True),\n",
    "    pd.get_dummies(df.year, drop_first = True)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped['pct_d_rgdp']\n",
    "x = df_prepped.drop(columns = 'pct_d_rgdp')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 2/3, random_state = 490)\n",
    "\n",
    "x_train_std = x_train.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n",
    "x_test_std  = x_test.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n",
    "\n",
    "x_train_std = sm.add_constant(x_train_std)\n",
    "x_test_std  = sm.add_constant(x_test_std)\n",
    "x_train     = sm.add_constant(x_train)\n",
    "x_test      = sm.add_constant(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at `lm.ElasticNet?` and \n",
    "```\n",
    "fit = sm.OLS(y_train, x_train)\n",
    "fit.fit_regularized?\n",
    "```\n",
    "Determine which coefficients are the same, but named differently.\n",
    "Specifically, $\\alpha$ and the weight on the different constraints (i.e. $||\\beta||_2$ and $||\\beta||_1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.OLS(y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const                0.000000e+00\n",
       "pos_net_jobs         0.000000e+00\n",
       "emp_estabs           0.000000e+00\n",
       "estabs_entry_rate    1.913649e-01\n",
       "estabs_exit_rate    -9.842778e-02\n",
       "pop                 -2.365903e-07\n",
       "pop_pct_black       -2.046507e-02\n",
       "pop_pct_hisp         1.434198e-02\n",
       "lfpr                 1.614383e-02\n",
       "density              0.000000e+00\n",
       "lower                0.000000e+00\n",
       "similar              0.000000e+00\n",
       "2003                 0.000000e+00\n",
       "2004                 0.000000e+00\n",
       "2005                 0.000000e+00\n",
       "2006                 0.000000e+00\n",
       "2007                 0.000000e+00\n",
       "2008                 0.000000e+00\n",
       "2009                 0.000000e+00\n",
       "2010                 0.000000e+00\n",
       "2011                 0.000000e+00\n",
       "2012                 0.000000e+00\n",
       "2013                 0.000000e+00\n",
       "2014                 0.000000e+00\n",
       "2015                 0.000000e+00\n",
       "2016                 0.000000e+00\n",
       "2017                 0.000000e+00\n",
       "2018                 0.000000e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_elasticnet = fit.fit_regularized(method = 'elastic_net', alpha = 1, L1_wt=1)\n",
    "fit_elasticnet.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 5-fold cross-validation grid search with a random state of 490. \n",
    "Identify the optimally tuned hyperparameters.\n",
    "Use this grid:\n",
    "```\n",
    "param_grid = {'alpha': 10.**np.arange(-5, -1, 1), \n",
    "              'l1_ratio': np.arange(0, 1, 0.1)}\n",
    "```\n",
    "You will get a warning message about convergence.\n",
    "We will discuss it after the workshop.\n",
    "Think about why it occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1162767.3929822615, tolerance: 253.23442781744671\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1130609.4852849494, tolerance: 246.96229539243058\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1140144.213104668, tolerance: 248.68990538784342\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1196542.6978201729, tolerance: 260.293554009637\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1161936.2646294914, tolerance: 252.92220066402035\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1162777.3637906245, tolerance: 253.23442781744671\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1130619.7805362665, tolerance: 246.96229539243058\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1140154.498421904, tolerance: 248.68990538784342\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1196552.9823314415, tolerance: 260.293554009637\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1161945.7997337463, tolerance: 252.92220066402035\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1162876.7918673037, tolerance: 253.23442781744671\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1130722.4397446292, tolerance: 246.96229539243058\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1140257.0040295953, tolerance: 248.68990538784342\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1196655.5038959682, tolerance: 260.293554009637\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1162040.9328701198, tolerance: 252.92220066402035\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1163845.8859214333, tolerance: 253.23442781744671\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1131722.5995664615, tolerance: 246.96229539243058\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1141251.169290107, tolerance: 248.68990538784342\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1197651.888232546, tolerance: 260.293554009637\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1162972.3791399468, tolerance: 252.92220066402035\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juli/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1449757.3812360945, tolerance: 315.5255958178445\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "param_grid = [{'alpha': 10.**np.arange(-5, -1, 1), 'l1_ratio': np.arange(0, 1, 0.1)}]\n",
    "cv_elasticnet = lm.ElasticNet(fit_intercept = False, normalize = False,\n",
    "                    random_state = 490)\n",
    "\n",
    "grid_search = GridSearchCV(cv_elasticnet, param_grid, cv = 5,\n",
    "                         scoring = 'neg_root_mean_squared_error')\n",
    "grid_search.fit(x_train_std, y_train)\n",
    "alpha_best = grid_search.best_params_['alpha']\n",
    "print(alpha_best)\n",
    "ratio_best = grid_search.best_params_['l1_ratio']\n",
    "print(ratio_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Question\n",
    "\n",
    "How many models did we just fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "200 models(5 folders * 4 alphas * 10 ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Using the tuned hyperparameters, fit your elastic net model with `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.96364974,  0.56560128, -0.1799134 ,  0.87161969, -0.52587028,\n",
       "       -0.10111834,  0.01932953,  0.31077548,  0.48842639, -0.00830354,\n",
       "        0.60185639,  0.25471323, -0.05717825, -0.12751556,  0.01240067,\n",
       "        0.38462281, -0.3780806 , -0.41076982, -0.59953967,  0.11248966,\n",
       "       -0.14950141, -0.44118467, -0.02749793, -0.36985377, -0.29046667,\n",
       "       -0.63804809, -0.2729354 , -0.12243805])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_elastic_tuned = sm.OLS(y_train, x_train_std).fit_regularized(L1_wt = ratio_best,alpha = alpha_best, method = \"elastic_net\")\n",
    "fit_elastic_tuned.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the selected features refit\n",
    "\n",
    "- the non-regularized model with standardized features\n",
    "- the non-regularized model with non-standardized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = pd.Series(fit_elastic_tuned.params, index = x_train_std.columns)\n",
    "beta.index[beta == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_std_trim = x_train_std.loc[:, ~x_train_std.columns.isin(beta.index[beta == 0])]\n",
    "x_test_std_trim = x_test_std.loc[:, ~x_test_std.columns.isin(beta.index[beta == 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.041</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>    <td>pct_d_rgdp</td>           <td>AIC:</td>         <td>246976.3293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-02-26 19:14</td>        <td>BIC:</td>         <td>247212.3930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>33889</td>        <td>Log-Likelihood:</td>   <td>-1.2346e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>27</td>           <td>F-statistic:</td>        <td>54.18</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>33861</td>      <td>Prob (F-statistic):</td>  <td>2.24e-285</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.041</td>            <td>Scale:</td>          <td>85.550</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>           <th>Coef.</th>  <th>Std.Err.</th>    <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>1.9833</td>   <td>0.0502</td>  <td>39.4735</td> <td>0.0000</td> <td>1.8848</td>  <td>2.0818</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pos_net_jobs</th>      <td>0.5720</td>   <td>0.0546</td>  <td>10.4783</td> <td>0.0000</td> <td>0.4650</td>  <td>0.6791</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp_estabs</th>        <td>-0.1838</td>  <td>0.0544</td>  <td>-3.3801</td> <td>0.0007</td> <td>-0.2903</td> <td>-0.0772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>estabs_entry_rate</th> <td>0.8761</td>   <td>0.0607</td>  <td>14.4365</td> <td>0.0000</td> <td>0.7572</td>  <td>0.9951</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>estabs_exit_rate</th>  <td>-0.5393</td>  <td>0.0583</td>  <td>-9.2538</td> <td>0.0000</td> <td>-0.6535</td> <td>-0.4250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pop</th>               <td>-0.1024</td>  <td>0.0568</td>  <td>-1.8034</td> <td>0.0713</td> <td>-0.2136</td> <td>0.0089</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pop_pct_black</th>     <td>0.0237</td>   <td>0.0573</td>  <td>0.4138</td>  <td>0.6790</td> <td>-0.0886</td> <td>0.1361</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pop_pct_hisp</th>      <td>0.3156</td>   <td>0.0523</td>  <td>6.0394</td>  <td>0.0000</td> <td>0.2132</td>  <td>0.4181</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lfpr</th>              <td>0.4893</td>   <td>0.0624</td>  <td>7.8430</td>  <td>0.0000</td> <td>0.3671</td>  <td>0.6116</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>density</th>           <td>-0.0080</td>  <td>0.0540</td>  <td>-0.1478</td> <td>0.8825</td> <td>-0.1138</td> <td>0.0978</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lower</th>             <td>0.6112</td>   <td>0.0684</td>  <td>8.9412</td>  <td>0.0000</td> <td>0.4772</td>  <td>0.7452</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>similar</th>           <td>0.2618</td>   <td>0.0591</td>  <td>4.4314</td>  <td>0.0000</td> <td>0.1460</td>  <td>0.3775</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2003</th>              <td>-0.0918</td>  <td>0.0697</td>  <td>-1.3165</td> <td>0.1880</td> <td>-0.2284</td> <td>0.0449</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2004</th>              <td>-0.1632</td>  <td>0.0698</td>  <td>-2.3385</td> <td>0.0194</td> <td>-0.2999</td> <td>-0.0264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2005</th>              <td>-0.0191</td>  <td>0.0728</td>  <td>-0.2623</td> <td>0.7931</td> <td>-0.1618</td> <td>0.1236</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2006</th>              <td>0.3570</td>   <td>0.0732</td>  <td>4.8782</td>  <td>0.0000</td> <td>0.2135</td>  <td>0.5004</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2007</th>              <td>-0.4158</td>  <td>0.0697</td>  <td>-5.9662</td> <td>0.0000</td> <td>-0.5524</td> <td>-0.2792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008</th>              <td>-0.4472</td>  <td>0.0697</td>  <td>-6.4138</td> <td>0.0000</td> <td>-0.5838</td> <td>-0.3105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2009</th>              <td>-0.6359</td>  <td>0.0707</td>  <td>-8.9950</td> <td>0.0000</td> <td>-0.7745</td> <td>-0.4973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2010</th>              <td>0.0809</td>   <td>0.0706</td>  <td>1.1470</td>  <td>0.2514</td> <td>-0.0574</td> <td>0.2193</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2011</th>              <td>-0.1847</td>  <td>0.0709</td>  <td>-2.6038</td> <td>0.0092</td> <td>-0.3237</td> <td>-0.0457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2012</th>              <td>-0.4797</td>  <td>0.0709</td>  <td>-6.7700</td> <td>0.0000</td> <td>-0.6186</td> <td>-0.3408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2013</th>              <td>-0.0619</td>  <td>0.0708</td>  <td>-0.8752</td> <td>0.3815</td> <td>-0.2007</td> <td>0.0768</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2014</th>              <td>-0.4090</td>  <td>0.0718</td>  <td>-5.7003</td> <td>0.0000</td> <td>-0.5496</td> <td>-0.2684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2015</th>              <td>-0.3281</td>  <td>0.0708</td>  <td>-4.6337</td> <td>0.0000</td> <td>-0.4669</td> <td>-0.1893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2016</th>              <td>-0.6795</td>  <td>0.0712</td>  <td>-9.5455</td> <td>0.0000</td> <td>-0.8191</td> <td>-0.5400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2017</th>              <td>-0.3102</td>  <td>0.0711</td>  <td>-4.3622</td> <td>0.0000</td> <td>-0.4495</td> <td>-0.1708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2018</th>              <td>-0.1586</td>  <td>0.0713</td>  <td>-2.2239</td> <td>0.0262</td> <td>-0.2983</td> <td>-0.0188</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>34608.272</td>  <td>Durbin-Watson:</td>       <td>1.994</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>10681061.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>4.484</td>       <td>Prob(JB):</td>         <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>89.509</td>    <td>Condition No.:</td>         <td>6</td>     \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                  Results: Ordinary least squares\n",
       "====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.041      \n",
       "Dependent Variable: pct_d_rgdp       AIC:                246976.3293\n",
       "Date:               2021-02-26 19:14 BIC:                247212.3930\n",
       "No. Observations:   33889            Log-Likelihood:     -1.2346e+05\n",
       "Df Model:           27               F-statistic:        54.18      \n",
       "Df Residuals:       33861            Prob (F-statistic): 2.24e-285  \n",
       "R-squared:          0.041            Scale:              85.550     \n",
       "--------------------------------------------------------------------\n",
       "                      Coef.  Std.Err.    t    P>|t|   [0.025  0.975]\n",
       "--------------------------------------------------------------------\n",
       "const                 1.9833   0.0502 39.4735 0.0000  1.8848  2.0818\n",
       "pos_net_jobs          0.5720   0.0546 10.4783 0.0000  0.4650  0.6791\n",
       "emp_estabs           -0.1838   0.0544 -3.3801 0.0007 -0.2903 -0.0772\n",
       "estabs_entry_rate     0.8761   0.0607 14.4365 0.0000  0.7572  0.9951\n",
       "estabs_exit_rate     -0.5393   0.0583 -9.2538 0.0000 -0.6535 -0.4250\n",
       "pop                  -0.1024   0.0568 -1.8034 0.0713 -0.2136  0.0089\n",
       "pop_pct_black         0.0237   0.0573  0.4138 0.6790 -0.0886  0.1361\n",
       "pop_pct_hisp          0.3156   0.0523  6.0394 0.0000  0.2132  0.4181\n",
       "lfpr                  0.4893   0.0624  7.8430 0.0000  0.3671  0.6116\n",
       "density              -0.0080   0.0540 -0.1478 0.8825 -0.1138  0.0978\n",
       "lower                 0.6112   0.0684  8.9412 0.0000  0.4772  0.7452\n",
       "similar               0.2618   0.0591  4.4314 0.0000  0.1460  0.3775\n",
       "2003                 -0.0918   0.0697 -1.3165 0.1880 -0.2284  0.0449\n",
       "2004                 -0.1632   0.0698 -2.3385 0.0194 -0.2999 -0.0264\n",
       "2005                 -0.0191   0.0728 -0.2623 0.7931 -0.1618  0.1236\n",
       "2006                  0.3570   0.0732  4.8782 0.0000  0.2135  0.5004\n",
       "2007                 -0.4158   0.0697 -5.9662 0.0000 -0.5524 -0.2792\n",
       "2008                 -0.4472   0.0697 -6.4138 0.0000 -0.5838 -0.3105\n",
       "2009                 -0.6359   0.0707 -8.9950 0.0000 -0.7745 -0.4973\n",
       "2010                  0.0809   0.0706  1.1470 0.2514 -0.0574  0.2193\n",
       "2011                 -0.1847   0.0709 -2.6038 0.0092 -0.3237 -0.0457\n",
       "2012                 -0.4797   0.0709 -6.7700 0.0000 -0.6186 -0.3408\n",
       "2013                 -0.0619   0.0708 -0.8752 0.3815 -0.2007  0.0768\n",
       "2014                 -0.4090   0.0718 -5.7003 0.0000 -0.5496 -0.2684\n",
       "2015                 -0.3281   0.0708 -4.6337 0.0000 -0.4669 -0.1893\n",
       "2016                 -0.6795   0.0712 -9.5455 0.0000 -0.8191 -0.5400\n",
       "2017                 -0.3102   0.0711 -4.3622 0.0000 -0.4495 -0.1708\n",
       "2018                 -0.1586   0.0713 -2.2239 0.0262 -0.2983 -0.0188\n",
       "--------------------------------------------------------------------\n",
       "Omnibus:            34608.272     Durbin-Watson:        1.994       \n",
       "Prob(Omnibus):      0.000         Jarque-Bera (JB):     10681061.763\n",
       "Skew:               4.484         Prob(JB):             0.000       \n",
       "Kurtosis:           89.509        Condition No.:        6           \n",
       "====================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_std_final = sm.OLS(y_train, x_train_std_trim).fit()\n",
    "fit_std_final.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trim = x_train.loc[:, ~x_train.columns.isin(beta.index[beta == 0])]\n",
    "x_test_trim = x_test.loc[:, ~x_test.columns.isin(beta.index[beta == 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.041</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>    <td>pct_d_rgdp</td>           <td>AIC:</td>         <td>246976.3293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-02-26 19:15</td>        <td>BIC:</td>         <td>247212.3930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>33889</td>        <td>Log-Likelihood:</td>   <td>-1.2346e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>27</td>           <td>F-statistic:</td>        <td>54.18</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>33861</td>      <td>Prob (F-statistic):</td>  <td>2.24e-285</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.041</td>            <td>Scale:</td>          <td>85.550</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>           <th>Coef.</th>  <th>Std.Err.</th>    <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>-2.1296</td>  <td>0.5863</td>  <td>-3.6321</td> <td>0.0003</td> <td>-3.2788</td> <td>-0.9804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pos_net_jobs</th>      <td>1.1525</td>   <td>0.1100</td>  <td>10.4783</td> <td>0.0000</td> <td>0.9369</td>  <td>1.3681</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp_estabs</th>        <td>-0.0385</td>  <td>0.0114</td>  <td>-3.3801</td> <td>0.0007</td> <td>-0.0608</td> <td>-0.0162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>estabs_entry_rate</th> <td>0.2894</td>   <td>0.0200</td>  <td>14.4365</td> <td>0.0000</td> <td>0.2501</td>  <td>0.3287</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>estabs_exit_rate</th>  <td>-0.2108</td>  <td>0.0228</td>  <td>-9.2538</td> <td>0.0000</td> <td>-0.2554</td> <td>-0.1661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pop</th>               <td>-0.0000</td>  <td>0.0000</td>  <td>-1.8034</td> <td>0.0713</td> <td>-0.0000</td> <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pop_pct_black</th>     <td>0.0016</td>   <td>0.0039</td>  <td>0.4138</td>  <td>0.6790</td> <td>-0.0061</td> <td>0.0093</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pop_pct_hisp</th>      <td>0.0241</td>   <td>0.0040</td>  <td>6.0394</td>  <td>0.0000</td> <td>0.0163</td>  <td>0.0319</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lfpr</th>              <td>0.0439</td>   <td>0.0056</td>  <td>7.8430</td>  <td>0.0000</td> <td>0.0329</td>  <td>0.0548</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>density</th>           <td>-0.0000</td>  <td>0.0000</td>  <td>-0.1478</td> <td>0.8825</td> <td>-0.0001</td> <td>0.0001</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lower</th>             <td>1.2597</td>   <td>0.1409</td>  <td>8.9412</td>  <td>0.0000</td> <td>0.9836</td>  <td>1.5358</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>similar</th>           <td>0.6765</td>   <td>0.1527</td>  <td>4.4314</td>  <td>0.0000</td> <td>0.3773</td>  <td>0.9757</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2003</th>              <td>-0.3885</td>  <td>0.2951</td>  <td>-1.3165</td> <td>0.1880</td> <td>-0.9668</td> <td>0.1899</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2004</th>              <td>-0.6965</td>  <td>0.2978</td>  <td>-2.3385</td> <td>0.0194</td> <td>-1.2803</td> <td>-0.1127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2005</th>              <td>-0.0808</td>  <td>0.3080</td>  <td>-0.2623</td> <td>0.7931</td> <td>-0.6846</td> <td>0.5230</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2006</th>              <td>1.5046</td>   <td>0.3084</td>  <td>4.8782</td>  <td>0.0000</td> <td>0.9001</td>  <td>2.1091</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2007</th>              <td>-1.7536</td>  <td>0.2939</td>  <td>-5.9662</td> <td>0.0000</td> <td>-2.3298</td> <td>-1.1775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008</th>              <td>-1.9061</td>  <td>0.2972</td>  <td>-6.4138</td> <td>0.0000</td> <td>-2.4886</td> <td>-1.3236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2009</th>              <td>-2.6897</td>  <td>0.2990</td>  <td>-8.9950</td> <td>0.0000</td> <td>-3.2758</td> <td>-2.1036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2010</th>              <td>0.3421</td>   <td>0.2983</td>  <td>1.1470</td>  <td>0.2514</td> <td>-0.2425</td> <td>0.9268</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2011</th>              <td>-0.7776</td>  <td>0.2986</td>  <td>-2.6038</td> <td>0.0092</td> <td>-1.3630</td> <td>-0.1923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2012</th>              <td>-2.0476</td>  <td>0.3024</td>  <td>-6.7700</td> <td>0.0000</td> <td>-2.6404</td> <td>-1.4548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2013</th>              <td>-0.2639</td>  <td>0.3015</td>  <td>-0.8752</td> <td>0.3815</td> <td>-0.8548</td> <td>0.3271</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2014</th>              <td>-1.7180</td>  <td>0.3014</td>  <td>-5.7003</td> <td>0.0000</td> <td>-2.3087</td> <td>-1.1273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2015</th>              <td>-1.4179</td>  <td>0.3060</td>  <td>-4.6337</td> <td>0.0000</td> <td>-2.0177</td> <td>-0.8181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2016</th>              <td>-2.8945</td>  <td>0.3032</td>  <td>-9.5455</td> <td>0.0000</td> <td>-3.4889</td> <td>-2.3002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2017</th>              <td>-1.3208</td>  <td>0.3028</td>  <td>-4.3622</td> <td>0.0000</td> <td>-1.9143</td> <td>-0.7274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2018</th>              <td>-0.6856</td>  <td>0.3083</td>  <td>-2.2239</td> <td>0.0262</td> <td>-1.2898</td> <td>-0.0813</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>34608.272</td>  <td>Durbin-Watson:</td>       <td>1.994</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>10681061.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>4.484</td>       <td>Prob(JB):</td>         <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>89.509</td>    <td>Condition No.:</td>      <td>6203501</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                  Results: Ordinary least squares\n",
       "====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.041      \n",
       "Dependent Variable: pct_d_rgdp       AIC:                246976.3293\n",
       "Date:               2021-02-26 19:15 BIC:                247212.3930\n",
       "No. Observations:   33889            Log-Likelihood:     -1.2346e+05\n",
       "Df Model:           27               F-statistic:        54.18      \n",
       "Df Residuals:       33861            Prob (F-statistic): 2.24e-285  \n",
       "R-squared:          0.041            Scale:              85.550     \n",
       "--------------------------------------------------------------------\n",
       "                      Coef.  Std.Err.    t    P>|t|   [0.025  0.975]\n",
       "--------------------------------------------------------------------\n",
       "const                -2.1296   0.5863 -3.6321 0.0003 -3.2788 -0.9804\n",
       "pos_net_jobs          1.1525   0.1100 10.4783 0.0000  0.9369  1.3681\n",
       "emp_estabs           -0.0385   0.0114 -3.3801 0.0007 -0.0608 -0.0162\n",
       "estabs_entry_rate     0.2894   0.0200 14.4365 0.0000  0.2501  0.3287\n",
       "estabs_exit_rate     -0.2108   0.0228 -9.2538 0.0000 -0.2554 -0.1661\n",
       "pop                  -0.0000   0.0000 -1.8034 0.0713 -0.0000  0.0000\n",
       "pop_pct_black         0.0016   0.0039  0.4138 0.6790 -0.0061  0.0093\n",
       "pop_pct_hisp          0.0241   0.0040  6.0394 0.0000  0.0163  0.0319\n",
       "lfpr                  0.0439   0.0056  7.8430 0.0000  0.0329  0.0548\n",
       "density              -0.0000   0.0000 -0.1478 0.8825 -0.0001  0.0001\n",
       "lower                 1.2597   0.1409  8.9412 0.0000  0.9836  1.5358\n",
       "similar               0.6765   0.1527  4.4314 0.0000  0.3773  0.9757\n",
       "2003                 -0.3885   0.2951 -1.3165 0.1880 -0.9668  0.1899\n",
       "2004                 -0.6965   0.2978 -2.3385 0.0194 -1.2803 -0.1127\n",
       "2005                 -0.0808   0.3080 -0.2623 0.7931 -0.6846  0.5230\n",
       "2006                  1.5046   0.3084  4.8782 0.0000  0.9001  2.1091\n",
       "2007                 -1.7536   0.2939 -5.9662 0.0000 -2.3298 -1.1775\n",
       "2008                 -1.9061   0.2972 -6.4138 0.0000 -2.4886 -1.3236\n",
       "2009                 -2.6897   0.2990 -8.9950 0.0000 -3.2758 -2.1036\n",
       "2010                  0.3421   0.2983  1.1470 0.2514 -0.2425  0.9268\n",
       "2011                 -0.7776   0.2986 -2.6038 0.0092 -1.3630 -0.1923\n",
       "2012                 -2.0476   0.3024 -6.7700 0.0000 -2.6404 -1.4548\n",
       "2013                 -0.2639   0.3015 -0.8752 0.3815 -0.8548  0.3271\n",
       "2014                 -1.7180   0.3014 -5.7003 0.0000 -2.3087 -1.1273\n",
       "2015                 -1.4179   0.3060 -4.6337 0.0000 -2.0177 -0.8181\n",
       "2016                 -2.8945   0.3032 -9.5455 0.0000 -3.4889 -2.3002\n",
       "2017                 -1.3208   0.3028 -4.3622 0.0000 -1.9143 -0.7274\n",
       "2018                 -0.6856   0.3083 -2.2239 0.0262 -1.2898 -0.0813\n",
       "--------------------------------------------------------------------\n",
       "Omnibus:            34608.272     Durbin-Watson:        1.994       \n",
       "Prob(Omnibus):      0.000         Jarque-Bera (JB):     10681061.763\n",
       "Skew:               4.484         Prob(JB):             0.000       \n",
       "Kurtosis:           89.509        Condition No.:        6203501     \n",
       "====================================================================\n",
       "* The condition number is large (6e+06). This might indicate\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_final = sm.OLS(y_train, x_train_trim).fit()\n",
    "fit_final.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the percent improvement from the null model RMSE to the elastic-net and OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.403229309446852"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_null = np.sqrt(np.mean(  (y_test - np.mean(y_train))**2  ))\n",
    "rmse_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.215633926070854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_elastic = np.sqrt(np.mean(  (y_test - fit_elastic_tuned.predict(x_test_std))**2  ))\n",
    "print(rmse_elastic)\n",
    "round((rmse_elastic - rmse_null)/rmse_null*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.215449872590813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_std_final = np.sqrt(np.mean(  (y_test - fit_std_final.predict(x_test_std_trim))**2  ))\n",
    "print(rmse_std_final)\n",
    "round((rmse_std_final - rmse_null)/rmse_null*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.215378381791666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_final = np.sqrt(np.mean(  (y_test - fit_final.predict(x_test_trim))**2  ))\n",
    "print(rmse_final)\n",
    "round((rmse_final - rmse_null)/rmse_null*100, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
